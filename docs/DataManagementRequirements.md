
# Data Management Module Requirements

## 1. Overview
The Data Management Module is a centralized interface designed to maintain the integrity, currency, and coverage of the trading data used by the system (Charts, Backtests, Profiler). It aims to replace ad-hoc scripts with a unified, safe, and transparent workflow.

## 2. Key Objectives
1.  **Centralization**: All data operations (import, update, verify) must be accessible from a single dashboard.
2.  **Visibility**: Users must clearly see the state of their data (Date ranges, missing gaps, last updated).
3.  **Safety**: **Zero Data Loss**. Existing data must never be corrupted. Backups must be automatic.
4.  **Automation**: Simple "One-Click" updates for disparate sources (Yahoo Finance, TradingView exports).

## 3. Functional Requirements

### 3.1. Data Status Dashboard
- **File Inventory Categories**:
    - The dashboard must organize files into tabs/categories to reduce clutter:
        - **Market Data**: Core Parquet files (e.g., `ES1_1m.parquet`).
        - **Derived Assets**: Generated analysis files (e.g., `*_profiler.json`).
        - **Other**: Backups and misc files.
- **Metadata Display**:
    - **Display Timezone**: **EST (America/New_York)**. All dates and times must be converted to EST for consistency with market hours.
    - **Start / End Date**: Extracted from the actual `datetime` column of the Parquet file (Min/Max statistics).
    - **Last Updated**: File modification time.
    - **Size & Rows**: Physical size and row count.
- **Health Check**: Identify gaps or stale data (e.g., "Profiler data is older than Parquet data").

### 3.2. Data Updating (Intraday)
- **Source**: Yahoo Finance (`yfinance`).
- **Scope**: Recent history (Last 7 days at 1m resolution).
- **Mechanism**:
    - Fetch recent data.
    - Load existing master Parquet.
    - Merge new data (deduplicate).
    - **Backup** existing file.
    - **Backup** existing file.
    - Write new file atomically.
- **Constraints & Strategy**:
    - **1-Minute Data**: Master Source. Updated via Yahoo Finance (last 5d).
    - **Derived Intraday (5m, 15m, 1h, 4h)**: Generated by **Upsampling** the 1m Master. This ensures consistency across timeframes and maximizes history depth.
    - **Daily/Weekly Data**: MUST be downloaded directly as `1d`/`1wk` intervals to capture official **Settlement Closes**. Do NOT aggregate 1m data to create Daily bars.

> [!WARNING]
> **Yahoo Finance Reliability Issue (Dec 2025)**
> Validated reports of "random candles" and bad ticks injected by Yahoo Finance auto-updates.
> **Action**: Auto-updates are currently SUSPENDED until validation logic is improved.
> Use NinjaTrader Import for clean data.

### 3.3. Data Import (Historical) - STANDARD OPERATING PROCEDURE (SOP)
This section defines the rigorous process for importing historical data from NinjaTrader exports.

#### **A. Source Format Standards**
- **Exports**: From NinjaTrader 8 "Export Data" functionality.
- **Columns**: `Date, Time, Open, High, Low, Close, Volume` (semicolon or comma delimited).
- **Timezone**: **Pacific Time** (America/Los_Angeles). This is the standard export TZ for our setup.
- **Timestamp Style**: **Close Time**. NinjaTrader provides the time a bar *closed*. We use **Open Time** (start of interval).
    - **Action**: All timestamps must be shifted BACK by the interval duration (e.g., -1 minute for 1m data).

#### **B. Import Workflow**

**1. Verify Alignment (Pre-Import Check)**
Before importing, ALWAYS verify that the new file aligns with existing data to prevent corruption.
```powershell
python scripts/verify_import_alignment.py "path/to/ES Monday.csv" ES1 --tz "America/Los_Angeles"
```
*Expectation*: Output should say "âœ… SUCCESS: Data is aligned."

**2. Execute Import & Merge**
Use the standard import script. This script automatically handles backups, timezone conversion, time shifting, and safe merging.
```powershell
python scripts/import_ninjatrader.py "path/to/ES Monday.csv" ES1 1m
```
*Note*: `1m` argument implies a 1-minute Close->Open shift.

**3. Verify Continuity**
Check for unexpected gaps in the data, specifically focusing on the newly imported era.
```powershell
python scripts/check_data_continuity.py ES1 --year 2008 --gap 48
```

#### **C. Derived Data Regeneration**
After *any* successful import, you **MUST** regenerate derived datasets to propagate the changes (HOD/LOD, Profiler stats, Web Chunks).
```powershell
python scripts/regenerate_derived.py ES1
```

## 4. Safety & Integrity Strategy

### 4.1. Backup Protocol
> **CRITICAL**: No write operation shall proceed without a verifiable backup.

- **Pre-Write Backup**: Before modifying `X.parquet`, copy it to `X.parquet.bak` (or `backups/X_YYYYMMDD.parquet`).
- **Atomic Writes**: Write new data to `X.parquet.tmp`, verify success, then rename to `X.parquet`.

### 4.2. Testing Strategy
A robust testing suite will be implemented before features go live:
1.  **Dry-Run Mode**: Scripts will have a flag to simulate operations and print what *would* happen (rows added, file size change) without writing to disk.
2.  **Integrity Checks**:
    - **Schema Validation**: Ensure columns match exactly.
    - **Date Monotonicity**: Ensure timestamps are strictly increasing.
    - **Duplicate Check**: Verify no duplicate timestamps exist after merge.

### 4.4. Cloud Backup (New)
- **Destinations**: Support for local cloud mounts:
    - **Google Drive**: `G:\My Drive\TradingBackups`
    - **OneDrive**: `C:\Users\{USER}\OneDrive\TradingBackups`
- **Mechanism**: `scripts/backup_to_gdrive.py` copies all Parquet files and Profiler JSONs to a timestamped folder.

## 5. Implementation Roadmap
1.  **Documentation & UI Skeleton**: visualize the dashboard without connecting dangerous logic.
2.  **Read-Only Status**: Implement the backend to *read* and *report* status (safe).
3.  **Backup System**: Implement and test the backup/restore utility (Local + Cloud).
4.  **Update Logic**: Implement `yfinance` updater with strict backup protocols.
5.  **Import Logic**:
    - **NinjaTrader**: `scripts/import_ninjatrader.py` parses NT8 CSV exports (`yyyyMMdd HHmmss;O;H;L;C;V`).
    - **Standardization**: System enforces `1W` (Weekly) and `60-day` (5m) limits.

### 3.4. Derived Data Regeneration
- **Objective**: Ensure all analysis files (Profiler, Levels, HOD/LOD) are consistent with the latest Market Data.
- **Workflow**:
    - Triggered automatically after **Import** or **Update**.
    - **Step 0: Upsample**: Generate 5m, 15m, 1h, 4h Parquet from 1m Master.
    - **Step 1: Daily HOD/LOD**: Run `precompute_daily_hod_lod.py` to generate `{ticker}_daily_hod_lod.json` (Price/Time Scatter).
    - **Step 2: Profiler Stats**: Run `precompute_daily_sessions.py` to generate `{ticker}_profiler.json`.
    - **Step 3: Level Touches**: Run `precompute_level_touches.py` to generate `{ticker}_level_touches.json`.
    - **Automation**: The master script `scripts/regenerate_derived.py` orchestrates this entire sequence.
